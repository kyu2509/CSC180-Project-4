{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports & Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections.abc import Sequence\n",
    "from sklearn import preprocessing\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import shutil\n",
    "import os\n",
    "\n",
    "import random\n",
    "from math import floor\n",
    "from matplotlib.pyplot import figure, show\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "from sklearn import metrics\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras import optimizers\n",
    "\n",
    "# Encode text values to dummy variables(i.e. [1,0,0],[0,1,0],[0,0,1] for red,green,blue)\n",
    "def encode_text_dummy(df, name):\n",
    "    dummies = pd.get_dummies(df[name])\n",
    "    for x in dummies.columns:\n",
    "        dummy_name = \"{}-{}\".format(name, x)\n",
    "        df[dummy_name] = dummies[x]\n",
    "    df.drop(name, axis=1, inplace=True)\n",
    "\n",
    "\n",
    "# Encode text values to indexes(i.e. [1],[2],[3] for red,green,blue).\n",
    "def encode_text_index(df, name):\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    df[name] = le.fit_transform(df[name])\n",
    "    return le.classes_\n",
    "\n",
    "\n",
    "# Encode a numeric column as zscores\n",
    "def encode_numeric_zscore(df, name, mean=None, sd=None):\n",
    "    if mean is None:\n",
    "        mean = df[name].mean()\n",
    "\n",
    "    if sd is None:\n",
    "        sd = df[name].std()\n",
    "\n",
    "    df[name] = (df[name] - mean) / sd\n",
    "\n",
    "\n",
    "# Convert all missing values in the specified column to the median\n",
    "def missing_median(df, name):\n",
    "    med = df[name].median()\n",
    "    df[name] = df[name].fillna(med)\n",
    "\n",
    "\n",
    "# Convert all missing values in the specified column to the default\n",
    "def missing_default(df, name, default_value):\n",
    "    df[name] = df[name].fillna(default_value)\n",
    "\n",
    "\n",
    "# Convert a Pandas dataframe to the x,y inputs that TensorFlow needs\n",
    "def to_xy(df, target):\n",
    "    result = []\n",
    "    for x in df.columns:\n",
    "        if x != target:\n",
    "            result.append(x)\n",
    "    # find out the type of the target column. \n",
    "    target_type = df[target].dtypes\n",
    "    target_type = target_type[0] if isinstance(target_type, Sequence) else target_type\n",
    "    # Encode to int for classification, float otherwise. TensorFlow likes 32 bits.\n",
    "    if target_type in (np.int64, np.int32):\n",
    "        # Classification\n",
    "        dummies = pd.get_dummies(df[target])\n",
    "        return df[result].values.astype(np.float32), dummies.values.astype(np.float32)\n",
    "    else:\n",
    "        # Regression\n",
    "        return df[result].values.astype(np.float32), df[target].values.astype(np.float32)\n",
    "\n",
    "# Nicely formatted time string\n",
    "def hms_string(sec_elapsed):\n",
    "    h = int(sec_elapsed / (60 * 60))\n",
    "    m = int((sec_elapsed % (60 * 60)) / 60)\n",
    "    s = sec_elapsed % 60\n",
    "    return \"{}:{:>02}:{:>05.2f}\".format(h, m, s)\n",
    "\n",
    "\n",
    "# Regression chart.\n",
    "def chart_regression(pred,y,sort=True):\n",
    "    t = pd.DataFrame({'pred' : pred, 'y' : y.flatten()})\n",
    "    if sort:\n",
    "        t.sort_values(by=['y'],inplace=True)\n",
    "    a = plt.plot(t['y'].tolist(),label='expected')\n",
    "    b = plt.plot(t['pred'].tolist(),label='prediction')\n",
    "    plt.ylabel('output')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# Remove all rows where the specified column is +/- sd standard deviations\n",
    "def remove_outliers(df, name, sd):\n",
    "    drop_rows = df.index[(np.abs(df[name] - df[name].mean()) >= (sd * df[name].std()))]\n",
    "    df.drop(drop_rows, axis=0, inplace=True)\n",
    "\n",
    "\n",
    "# Encode a column to a range between normalized_low and normalized_high.\n",
    "def encode_numeric_range(df, name, normalized_low=-1, normalized_high=1,\n",
    "                         data_low=None, data_high=None):\n",
    "    if data_low is None:\n",
    "        data_low = min(df[name])\n",
    "        data_high = max(df[name])\n",
    "\n",
    "    df[name] = ((df[name] - data_low) / (data_high - data_low)) \\\n",
    "               * (normalized_high - normalized_low) + normalized_low\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Katrina Yu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3578, 7)\n",
      "         Date       Open       High        Low      Close  Adj Close    Volume\n",
      "0  2004-08-19  49.676899  51.693783  47.669952  49.845802  49.845802  44994500\n",
      "1  2004-08-20  50.178635  54.187561  49.925285  53.805050  53.805050  23005800\n",
      "2  2004-08-23  55.017166  56.373344  54.172661  54.346527  54.346527  18393200\n",
      "3  2004-08-24  55.260582  55.439419  51.450363  52.096165  52.096165  15361800\n",
      "4  2004-08-25  52.140873  53.651051  51.604362  52.657513  52.657513   9257400\n",
      "(3578, 6)\n",
      "        Open       High        Low   In Close    Volume  Out Close\n",
      "0  49.676899  51.693783  47.669952  49.845802  44994500  49.845802\n",
      "1  50.178635  54.187561  49.925285  53.805050  23005800  53.805050\n",
      "2  55.017166  56.373344  54.172661  54.346527  18393200  54.346527\n",
      "3  55.260582  55.439419  51.450363  52.096165  15361800  52.096165\n",
      "4  52.140873  53.651051  51.604362  52.657513   9257400  52.657513\n",
      "In Close:\n",
      "\n",
      "         In Close\n",
      "0       49.845802\n",
      "1       53.805050\n",
      "2       54.346527\n",
      "3       52.096165\n",
      "4       52.657513\n",
      "...           ...\n",
      "3573  1071.469971\n",
      "3574  1020.080017\n",
      "3575  1036.209961\n",
      "3576  1076.770020\n",
      "3577  1070.000000\n",
      "\n",
      "[3578 rows x 1 columns]\n",
      "Out Close:\n",
      "\n",
      "        Out Close\n",
      "0       49.845802\n",
      "1       53.805050\n",
      "2       54.346527\n",
      "3       52.096165\n",
      "4       52.657513\n",
      "...           ...\n",
      "3573  1071.469971\n",
      "3574  1020.080017\n",
      "3575  1036.209961\n",
      "3576  1076.770020\n",
      "3577  1070.000000\n",
      "\n",
      "[3578 rows x 1 columns]\n",
      "        Open       High        Low   In Close    Volume  Out Close\n",
      "0  49.676899  51.693783  47.669952  49.845802  44994500  49.845802\n",
      "1  50.178635  54.187561  49.925285  53.805050  23005800  53.805050\n",
      "2  55.017166  56.373344  54.172661  54.346527  18393200  54.346527\n",
      "3  55.260582  55.439419  51.450363  52.096165  15361800  52.096165\n",
      "4  52.140873  53.651051  51.604362  52.657513   9257400  52.657513\n"
     ]
    }
   ],
   "source": [
    "path = \"./data/\"\n",
    "\n",
    "df = pd.read_csv(path + \"GOOG.csv\")\n",
    "\n",
    "print(df.shape)\n",
    "print(df[0:5])\n",
    "\n",
    "# Dropping date and Adj Close, not needed\n",
    "df.drop(columns={\"Date\", \"Adj Close\"}, inplace=True)\n",
    "\n",
    "out_close = df[[\"Close\"]]\n",
    "\n",
    "# Close Feature for Input\n",
    "df.rename(columns={\"Close\": \"In Close\"}, inplace=True)\n",
    "df = pd.concat([df, out_close], axis=1)\n",
    "\n",
    "# Close Feature for Output\n",
    "df.rename(columns={\"Close\": \"Out Close\"}, inplace=True)\n",
    "\n",
    "print(df.shape)\n",
    "print(df[0:5])\n",
    "\n",
    "# Close Feature for Input\n",
    "print(\"In Close:\\n\")\n",
    "print(df[[\"In Close\"][0:5]])\n",
    "\n",
    "# Close Feature for Output\n",
    "print(\"Out Close:\\n\")\n",
    "print(df[[\"Out Close\"][0:5]])\n",
    "\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalizing Data\n",
    "Katrina Yu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Open       High        Low   In Close    Volume  Out Close\n",
      "0  49.676899  51.693783  47.669952  49.845802  44994500  49.845802\n",
      "1  50.178635  54.187561  49.925285  53.805050  23005800  53.805050\n",
      "2  55.017166  56.373344  54.172661  54.346527  18393200  54.346527\n",
      "3  55.260582  55.439419  51.450363  52.096165  15361800  52.096165\n",
      "4  52.140873  53.651051  51.604362  52.657513   9257400  52.657513\n",
      "(3578, 6)\n",
      "       Open      High       Low  In Close    Volume  Out Close\n",
      "0 -1.334844 -1.331443 -1.337457 -1.333645  4.533247  49.845802\n",
      "1 -1.333101 -1.322843 -1.329562 -1.319893  1.870138  53.805050\n",
      "2 -1.316291 -1.315305 -1.314693 -1.318012  1.311494  54.346527\n",
      "3 -1.315446 -1.318526 -1.324223 -1.325828  0.944353  52.096165\n",
      "4 -1.326284 -1.324693 -1.323684 -1.323879  0.205033  52.657513\n"
     ]
    }
   ],
   "source": [
    "print(df.head())\n",
    "\n",
    "# Numeric Features\n",
    "numeric_feats = [\"Open\", \"High\", \"Low\", \"In Close\", \"Volume\"]\n",
    "\n",
    "# Data Normalization for Input Features, so exclude \"Out Close\"\n",
    "for i in numeric_feats:\n",
    "    encode_numeric_zscore(df, i)\n",
    "    \n",
    "print(df.shape)\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train-Test Split for Training & Testing\n",
    "Katrina Yu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2504, 6)\n",
      "(1074, 6)\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "x,y = to_xy(df, \"Out Close\")\n",
    "\n",
    "print(x, x.shape)\n",
    "print(y, y.shape) \n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.30, random_state=42)\n",
    "\n",
    "print(x_train.shape, y_train.shape)\n",
    "print(x_test.shape, y_test.shape)\n",
    "\n",
    "print(\"shape: \\n\")\n",
    "print(df[\"Open\"].shape[0])\n",
    "'''\n",
    "# Splitting 70% of data for Training & 30% for Testing\n",
    "split_index = floor((df[\"Open\"].shape[0])*0.70)\n",
    "\n",
    "df_train = df[0:split_index]\n",
    "df_test = df[split_index:]\n",
    "\n",
    "print(df_train.shape)\n",
    "print(df_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sequence data to create x and y in the format RNN likes.\n",
    "Katrina Yu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def to_sequences(seq_size, data):\n",
    "    x = []\n",
    "    y = []\n",
    "\n",
    "    for i in range(len(data)-seq_size-1):\n",
    "        #print(i)\n",
    "        window = data.iloc[i:(i+seq_size)].values\n",
    "        after_window = data.iloc[i+seq_size][\"Out Close\"]\n",
    "        #window = [[x] for x in window]\n",
    "        #print(\"{} - {}\".format(window,after_window))\n",
    "        x.append(window)\n",
    "        y.append(after_window)\n",
    "        \n",
    "    return np.array(x),np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2504\n",
      "Shape of x_train: (2496, 7, 6)\n",
      "Shape of x_test: (1066, 7, 6)\n",
      "Shape of y_train: (2496,)\n",
      "Shape of y_test: (1066,)\n"
     ]
    }
   ],
   "source": [
    "print(len(df_train))\n",
    "\n",
    "# Past 7 days used for prediction\n",
    "SEQUENCE_SIZE = 7\n",
    "x_train,y_train = to_sequences(SEQUENCE_SIZE, df_train)\n",
    "x_test,y_test = to_sequences(SEQUENCE_SIZE, df_test)\n",
    "\n",
    "print(\"Shape of x_train: {}\".format(x_train.shape))\n",
    "print(\"Shape of x_test: {}\".format(x_test.shape))\n",
    "print(\"Shape of y_train: {}\".format(y_train.shape))\n",
    "print(\"Shape of y_test: {}\".format(y_test.shape))\n",
    "\n",
    "# print(\"x_train:\\n {}\".format(x_train[0:3]))\n",
    "\n",
    "# print(\"y_train:\\n {}\".format(y_train[0:3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building the First RNN Model\n",
    "Katrina Yu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
